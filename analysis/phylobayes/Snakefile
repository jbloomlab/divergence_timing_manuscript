"""
Pipeline for `phylobayes` analysis.
SKH 20180117
"""
import glob
import Bio.SeqIO
import pandas as pd
from phydmslib import constants
import pandas as pd
import numpy as np
from pymodules.utils import *
import dms_tools2.plot


# Globals ---------------------------------------------------------------------
# Directories
ALIGNMENT_DIR = "../HA/data/subsample/"
PHYDMS_DIR = "../HA/branch_lengths/phydms/"

# Alignment IDs
ALIGNMENT_IDS = ["_".join(os.path.basename(x).split(".")[0].split("_")[1:])
                 for x in glob.glob("{0}/*.fasta".format(ALIGNMENT_DIR))]
SEEDS = [int(x.split("_")[-1]) for x in ALIGNMENT_IDS]
LEVELS = ["high", "intermediate", "low"]

# `phylobayes` params
N_STEPS = 5000
BURNIN = int(N_STEPS * 0.1)

TARGET1 = "cds_AAD17229_A/South_Carolina/1/1918_1918//_HA_HA_1"
TARGET2 = "cds_ABU50586_A/Solomon_Islands/3/2006_2006/08/21_HA_HA_1"
TARGET = [TARGET1, TARGET2]
TARGET.sort()
TARGET = "{0}_{1}".format(TARGET[0], TARGET[1])

AVG_PREFS = "../HA/data/references/HA_average_prefs.csv"
H1_PREFS = "../HA/data/references/HA_hybridDoud_prefs.csv"
H3_PREFS = "../HA/data/references/HA_hybridLee_prefs.csv"
AVG_BETA = 1.7
H1_BETA = 1.12
H3_BETA = 1.28
# Rules -----------------------------------------------------------------------

rule all:
    input:
        expand("outputs/{aligment_id}_prefs.csv", aligment_id=ALIGNMENT_IDS),
        "outputs/branch_lengths.csv",
        "outputs/branch_lengths_norm.csv",
        "outputs/branch_lengths_norm_all.csv",
        "outputs/model_by_model.csv",
        "outputs/corr_prefs.csv"

rule create_phylip_files:
    """
    This rule converts the fasta files from `HA/branch_lengths` into a
    `phylip` and changes the naming in the `newick` tree. The rule also
    outputs a map for old<->new name conversion
    """
    message: "Creating phylip file"
    input:
        alignment = ALIGNMENT_DIR + "HA_{alignment_id}.fasta",
        tree = PHYDMS_DIR + "{alignment_id}_RAxML_tree.newick"
    output:
        alignment = "_{alignment_id}_rename.phylip",
        tree = "_{alignment_id}_rename.newick",
        map = "_map_{alignment_id}.csv"
    run:
        df = {}
        final_seqs = []
        counter = 0
        for seq in Bio.SeqIO.parse(input.alignment, "fasta"):
            new_name = "SEQ_{0}".format(counter)
            df[seq.id] = new_name
            seq.id = new_name
            final_seqs.append(seq)
            counter += 1
        Bio.SeqIO.write(final_seqs, open(output.alignment, 'w'), 'phylip')

        # now rename the tree
        with open(input.tree, "r") as f:
            tree = f.read()
        for old_name in df.keys():
            tree = tree.replace(old_name, df[old_name])
        with open(output.tree, "w") as f:
            f.write(tree)

        # output map
        new_df = {"old_name": [], "new_name": []}
        for key in df.keys():
            new_df["old_name"].append(key)
            new_df["new_name"].append(df[key])
        new_df = pd.DataFrame(new_df)
        new_df.to_csv(output.map, index=False)

rule create_sbatch:
    """
    This rule creates an sbatch file to run `phylobayes`.
    """
    message: "Creating sbatch file"
    input:
        alignment = "_{alignment_id}_rename.phylip",
        tree = "_{alignment_id}_rename.newick",
    output:
        sbatch = "_{alignment_id}.sbatch"
    params:
        alignment_id = "_{alignment_id}"
    run:
        t = ("#!/bin/bash\n"
             "#SBATCH\n"
             "#SBATCH -o {0}.out\n"
             "#SBATCH -e {0}.err\n"
             "#SBATCH -p campus\n"
             "#SBATCH -n 16\n"
             "#SBATCH -t 5-00:00:00\n\n"
             "module load OpenMPI/1.8.4-GCC-4.9.2\n\n"
             "mpirun /home/mrg/Work/phylobayes/pbmpi/data/pb_mpi -x 1 {1}"
             " -mutsel -d {3} -T {4} -dp {0}\n"
             "/home/mrg/Work/phylobayes/pbmpi/data/readpb_mpi -x {2} {0}\n"
             "/home/mrg/Work/phylobayes/pbmpi/data/bpcomp -x {2} 5 {0}\n"
             "touch _done{0}.txt"
             .format(params.alignment_id, N_STEPS, BURNIN, input.alignment,
                     input.tree)
             )
        with open(output.sbatch, "w") as f:
            f.write(t)

rule run_sbatch:
    """
    This rule submits the sbatch files
    """
    message: "Running sbatch"
    input:
        sbatch = "_{alignment_id}.sbatch"
    output:
        monitor = "_done_{alignment_id}.txt"
    shell:
        "sbatch {input.sbatch}"

rule create_prefs:
    """
    This rule creates `phydms` formatted "preference" files from the
    `phylobayes` outputs
    """
    message: "Creating prefs"
    input:
        aap = "_{aligment_id}.aap"
    output:
        prefs = "outputs/{aligment_id}_prefs.csv"
    run:
        aas = list(constants.AA_TO_INDEX.keys())
        aas.sort()  # ABC order
        df = pd.read_csv(input.aap, sep="\t", skiprows=1, header=None)
        df = df.dropna(axis=1, how='all')  # sometimes is an all NA col
        df.columns = aas
        assert np.allclose(df.sum(axis=1),
                           pd.Series([1 for x in range(len(df))]))
        df["site"] = [x+1 for x in range(len(df))]
        final_cols = ["site"] + aas
        df = df[final_cols]
        df.to_csv(output.prefs, index=False)

# Make summary files
rule calc_branch_lengths:
    """
    The purpose of this rule is to cacluate the distance between all pairs of
    tips on the `phylobayes` trees.
    """
    message: "Calculate branch lengths"
    input:
        pb = expand("_{alignment_id}.con.tre", alignment_id=ALIGNMENT_IDS),
        maps = expand("_map_{alignment_id}.csv", alignment_id=ALIGNMENT_IDS)
    output:
        summary = "outputs/branch_lengths.csv"
    run:
        final_cols = ["seq_id", "sequence1", "sequence2", "model", "distance",
                      "alignment_id", "alignment_ref", "divergence_level",
                      "from_ref"]
        final = []
        for run in input.pb:
            model_name = "pb"
            alignment_id = "_".join(os.path.basename(run).split(".")[0]
                                    .split("_")[1:])
            # set up map to change sequence names
            m = pd.read_csv("_map_{0}.csv".format(alignment_id))
            m = dict(zip(m['new_name'], m['old_name']))
            # process alignments
            df = calc_distances(run)
            df["model"] = [model_name for x in range(len(df))]
            df["alignment_id"] = [alignment_id for x in range(len(df))]
            df["alignment_ref"] = [alignment_id.split("_")[0] for x in
                                   range(len(df))]
            df["divergence_level"] = [alignment_id.split("_")[1]
                                      for x in range(len(df))]
            df["from_ref"] = ['WSN' if "Wilson-Smith" in x else 'Perth'
                              if "Perth" in x else 'no' for x
                              in df["seq_id"]]
            # fix the naming
            df["sequence1"] = [m[x] for x in df["sequence1"]]
            df["sequence2"] = [m[x] for x in df["sequence2"]]
            for index, row in df.iterrows():
                temp = [row["sequence1"], row["sequence2"]]
                temp.sort()
                df.set_value(index, 'seq_id',
                             "{0}_{1}".format(temp[0], temp[1]))
            final.append(df)
        # make final dataframe
        final = pd.concat(final)
        final = final[final_cols]
        final.to_csv(output.summary, index=False)

rule norm_branch_lengths:
    """
    The purpose of this rule is to normalize all of the branch lengths
    """
    message: "Norm branch lengths"
    input:
        branch_lengths = "outputs/branch_lengths.csv"
    output:
        norm = "outputs/branch_lengths_norm.csv"
    run:
        bl = pd.read_csv(input.branch_lengths)
        final = []
        for name, group in bl.groupby(["alignment_id"]):
            if TARGET in group["seq_id"].tolist():
                norm = group[group["seq_id"] == TARGET]["distance"]
                assert len(norm) == 1
                norm = norm.iloc[0]
                group["distance"] = [x/norm for x in group["distance"]]
                final.append(group)
        final = pd.concat(final)
        final.to_csv(output.norm, index=False)

rule add_in_old_data:
    """
    The purpose of this rule is to normalize the ExpCM data and add it to
    the `phylobayes` data
    """
    message: "Normalize and add in ExpCM data"
    input:
        expcm = "../HA/branch_lengths/outputs/branch_lengths.csv",
        pb = "outputs/branch_lengths_norm.csv"
    output:
        norm = "outputs/branch_lengths_norm_all.csv"
    run:
        expcm = pd.read_csv(input.expcm)
        final = []
        for n, group in expcm.groupby(["alignment_id", "model", "pref_set"]):
            if TARGET in group["seq_id"].tolist():
                norm = group[group["seq_id"] == TARGET]["distance"]
                assert len(norm) == 1
                norm = norm.iloc[0]
                group["distance"] = [x/norm for x in group["distance"]]
                final.append(group)
        expcm = pd.concat(final)

        for index, row in expcm.iterrows():
            new_model_name = "{0}_{1}".format(row["model"], row["pref_set"])
            expcm.set_value(index, 'model', new_model_name)
        pb = pd.read_csv(input.pb)
        pb = pd.concat([expcm, pb])
        pb.to_csv(output.norm, index=False)

rule make_model_by_model_norm:
    """
    The purpose of this rule is to reformat the branch lengths summary file
    to make it easy for plotting.
    """
    message: "Creating model by model"
    input:
        branch_lengths = "outputs/branch_lengths_norm_all.csv"
    output:
        model_by_model = "outputs/model_by_model.csv"
    run:
        final = []
        model_pairs = {
                       "pb": ["ExpCM_average", "ExpCM_gammaomega_average",
                              "ExpCM_Doud", "ExpCM_gammaomega_Doud",
                              "ExpCM_Lee", "ExpCM_gammaomega_Lee"],
                       }
        df = pd.read_csv(input.branch_lengths)
        for alignment, alignment_df in df.groupby(["alignment_id"]):
            for model_1, model_1_df in alignment_df.groupby(["model"]):
                if model_1 in list(model_pairs.keys()):
                    first = model_1_df[["seq_id", "distance"]]
                    for model_2, temp in alignment_df.groupby(["model"]):
                        if model_2 in model_pairs[model_1]:
                            model_pair = "{0}/{1}".format(model_1, model_2)
                            temp["model_pair"] = [model_pair for x in
                                                  range(len(temp))]
                            temp = temp.rename(columns={"distance":
                                                        "distance2"})
                            temp = temp.drop(['model'], axis=1)
                            temp = temp.merge(first, on=['seq_id'])
                            temp = temp.rename(columns={"distance":
                                                        "distance1"})
                            final.append(temp)
        final = pd.concat(final)
        final.to_csv(output.model_by_model, index=False)

rule make_corr_prefs:
    """
    The purpose of this rule is to make a summary file with the preferences
    from phylobayes, H1, H3, and H1+H3
    """
    message: "Creating prefs correlation summary file"
    input:
        pb_prefs = expand("outputs/{aligment_id}_prefs.csv",
                          aligment_id=ALIGNMENT_IDS)
    output:
        corr = "outputs/corr_prefs.csv"
    run:
        doud = melt_df(pd.read_csv(H1_PREFS), "doud")
        doud_rescaled = melt_df(rescale_prefs(pd.read_csv(H1_PREFS), H1_BETA),
                                "doud_rescaled")
        final = pd.merge(doud, doud_rescaled, on=['site', 'amino_acid'])

        lee = melt_df(pd.read_csv(H3_PREFS), "lee")
        lee_rescaled = melt_df(rescale_prefs(pd.read_csv(H3_PREFS), H3_BETA),
                               "lee_rescaled")
        result = pd.merge(lee, lee_rescaled, on=['site', 'amino_acid'])
        final = pd.merge(final, result, on=['site', 'amino_acid'])

        avg = melt_df(pd.read_csv(AVG_PREFS), "avg")
        avg_rescaled = melt_df(rescale_prefs(pd.read_csv(AVG_PREFS), AVG_BETA),
                               "avg_rescaled")
        result = pd.merge(avg, avg_rescaled, on=['site', 'amino_acid'])
        final = pd.merge(final, result, on=['site', 'amino_acid'])

        for pb in input.pb_prefs:
            new_name = "mutSel_{0}".format(pb.split("_")[-3])
            profiles = melt_df(pd.read_csv(pb), new_name)
            final = pd.merge(final, profiles, on=['site', 'amino_acid'])

        final.to_csv(output.corr, index=False)
